### 数据整理(Data Wrangling)

就是做到将一个类型的数据转换为另一个，比如将m4s转换为mp4

或者是做到筛选一些数据，就像我们在grep里面做的那样，那是会更加精巧

#### 使用正则表达式(regular expression)：目的就是要在语句中按顺序找到特定的字符

1.基本匹配：就是abc就是匹配abc，一字一句的重复

2.元字符匹配

> - .匹配任意字符
> - ^匹配首位，$匹配末位，像是一种位置控制
> - \w匹配所有数字与字母
>
> - \W就是[\^\\w]
>
> - \d匹配digit，大写也是匹配非digit
>
> - \s匹配非空格符

3.字符集

> - \[abc\]非贪婪的匹配括号内任意一个字符
> - \[^abc\]匹配除了abc的其他所有一个字符
> - [A-Za-z0-9]都是表示一个字符

4.模式量词

> - *重复匹配零次或者多次，+就是匹配一次以上，比如a\*可以匹配aaaaa，也可以匹配无字符
> - ?指明一个非贪婪字符，就是说ab?c，b可有可无，比上面的\*范围更小
> - |就是或的意思，比如ab|dc就是ab或者dc，如果是a(b|d)c就是abc或者adc，注意括号的位
> - {n,m}最少n次最多m次，比如a{2，6}范围就是aa到aaaaaa
> - (xyz)按字符顺序准确匹配xyz

5.断言

> 就是我给一个词，但是这个词前后一定是某个能够断定正确与否的词
>
> 比如abc efg和abc www，我能够肯定abc的后面一定是跟着 efg
>
> | 符号 | 描述         |
> | :--- | :----------- |
> | ?=   | 正向先行断言 |
> | ?!   | 负向先行断言 |
> | ?<=  | 正向后行断言 |
> | ?<!  | 负向后行断言 |

例如正则表达式 `(T|t)he(?=\sfat)`，表示：匹配大写字母 `T` 或小写字母 `t`，后面跟字母 `h`，后跟字母 `e`。，在括号中，我们定义了正向先行断言，它会引导正则表达式引擎匹配后面跟着 `fat` 的 `The` 或 `the`。

6.分组

> 我们需要对一些特定的词组类型进行操作就需要对他们进行分组
>
> 比如(a)-\1就是第一组，(b)-\2就是第二组，但是如果我们要默认失效一个组，这时就在括号内添加(?:)，比如(?:ha)他就不会被捕获成一个特定的组

7.回溯

举个例子，如果我们使用c.*t来匹配cat，如果单纯使用我们脑海中的按顺序匹配的话，c匹配c，.\*匹配任意长度的字符就是匹配at，那么t就不匹配咯

但是reg的模式更加复杂，它发现不匹配了，他会将这个进行回溯，使得.\*匹配少的字符，所以存在多个可能，只要一种匹配成功就可以

**拓展：正则表达式的工作原理和python使用**

首先分析正则表达式的模式，将其编译为一系列数据结构，统称为状态机，然后运行编译完的程序进行尝试在文本中找到最佳匹配

> 最佳匹配基本上就是最长的匹配:比如在s is a toss里面，s.*s就会匹配整个句子而不是s is

并且我们可以在python中使用reg的匹配，比如下面的电话号码匹配

```python
import re
pattern = r'\d\d\d-\d\d\d-\d\d\d\d'
s = input('enter your tel number: ')
if re.match(pattern, s):
    print('number accepted')
else:
    print('incorrect format')
```

但是很重要的，只要是电话号码中有这个结构就可以被录入，比如555-555-5555000也可以被录入，所以可以在末尾添加$，或者将re.match改成re.fullmatch

如果需要多次使用同一个表达式，我们在python中可以用名称来代替它，很像shell中的alias

```python
import re
reg1 = re.compile(r'表达式')
```

#### 使用到的工具

##### grep：找到特定词存在的句子

在grep中这样使用正则表达式，使用-E启用正则模式

```bash
grep -E "[a-z]+" filename
```

其他的问tldr吧，比如-o(只输出匹配的文本)，-c(统计出现的文本行数)，-n(打印所在行号)，--R -n(组合形成递归搜索)

##### sed：替换文本

sed就是流编辑器，最常用在文本替换，而不是grep的文本输出，见题目

##### awk高级文本处理

awk可以处理数据流，他的脚本模式是

```bash
awk 'BEGIN{start} pattern {command} END{end}' file
```

就是对file文件进行操作，然后先进行begin之后的命令，其次对file中的每一行操作，如果能够符合pattern则执行command，最后是end的效果，其中begin、end和pattern都是可选的



#### 作业

**2.统计word文件里面的词，这些词满足:至少三个a但不以's结尾的单词个数**

```bash
grep word | tr "[:upper:]" "[:lower:]" | grep -E "^([^a]*a){3}.*$" | grep -v "'s$" -c
```

其中tr用于切换大小写:

> tr就是translate用于文本替换、删除和压缩，命令行格式为tr [options] set1 set2，来自stdin的输入会根据set1和set2格式来转换文本，比如tr 'A-Z' 'a-z'，本质上是一种集合映射

我们来研究一下这个正则表达式，首先^和$匹配开头和结尾，查找以a结尾的字符串，匹配\[^a]0次，但是包含a结尾的词块3次，接下来是任意字符，后面的grep -v 就是反向查找，以’a结尾的我不要

**共存在多少种词尾组合，最多的是哪些**

```bash
 sed -E 's/.*(\w{2})$/\1/g' word | sort | uniq -c | sort -r
```

解释一下：sed是替换命令，结构就是**'s/匹配正则表达式/你要替换的内容/g'**,sort就是将标准输入按字典序输出给**uniq -c就是把连续出现的行折叠成一行**并将出现次数换做前缀，然后sort -r按反序排序就给出了词尾组合，当然可以继续通过再加**wc -l**来打印行数

**哪些26*26的字母组合没有出现过**

我首先使用了一个sh脚本来输出一个全字符排列

```bash
for i in {a..z};do
	for j in {a..z};do
		echo $i$j >> file1.txt
	done;
done;
```

然后使用命令输出单词表的末尾字符

```bash
 sed -E 's/.*(\w{2})$/\1/g' word | sed -E 's/\w*\s(\w{2})$/\1/g' | sort | uniq > file1.txt
```

最后面用diff命令来显示不一样的地方

```bash
diff file1.txt file2.txt
```

**3.进行原地替换听上去很有诱惑力，例如： `sed s/REGEX/SUBSTITUTION/ input.txt > input.txt`。但是这并不是一个明智的做法，为什么呢？还是说只有 `sed`是这样的? 查看 `man sed` 来完成这个问题**（好的，一看就是上面的作业全不明智）

我不会，但是答案的意思就是input.txt会被先清空，但是此时还没有进行sed替换，所以可以先备份一下，使用备份文件进行替换，其中bak就是备份文件

```bash
sed -i.bak s/REGEX/SUBSTITUTION/ input.txt
```

**4.找出最近wsl开机的开机时间平均数，中位数和最长时间，需要用到journalctl**

我们可以先试一下journalctl，发现是非常的长，所有的系统数据都被保存在里面了，然后我将它输出到machine.log发现有500000那么多的数据,类似于下面的

```
May 22 18:37:08 LAPTOP-20FI3R31 systemd[1]:etc
```

所以说我们需要的就是前面的May 22 18:37:08

```bash
sed -i -E 's/^(\w*\s\d*\s\d{2}:\d{2}:\d{2}).*$/\1/g' machine.log  
```

接下来就是喜闻乐见的排序输出脚本的细节了

**答案**：但是答案给出了更加好的解决方案，我这个解决方案在一些机子里完成不了，试了很久，但是正则测试是对的

编写脚本

```bash
#!/bin/bash
for i in {0..9}; do
   journalctl -b-$i | grep "Startup finished in"
done
```

倒序输出journalctl的十条时间，然后计算

```bash
cat starttime.txt | grep "systemd\[1\]" | sed -E "s/.*=\ (.*)s\.$/\1/"| sort | tail -n1
cat starttime.txt | grep "systemd\[1\]" | sed -E "s/.*=\ (.*)s\.$/\1/"| sort -r | tail -n1
cat starttime.txt | grep "systemd\[1\]" | sed -E "s/.*=\ (.*)s\.$/\1/"| paste -sd+ | bc -l | awk '{print $1/10}'
cat starttime.txt | grep "systemd\[1\]" | sed -E "s/.*=\ (.*)s\.$/\1/"| sort |paste -sd\  | awk '{print ($5+$6)/2}'
```

**5.这个我们电脑没有，diff的部分就是laptop后面的部分**

**6.使用curl获取数据集，并且拿到其中两列数据，找出其中一列的最大值和最小值**

删除一些不必要的数据后，我们获得的html表格形式是如下的形式

```html
<tr><td class=rb>Sep&nbsp;2018</td><td class=rb>210989</td><td class=rb>96544</td><td class=rb><font color=#0000A0><b>66574</b></font></td><td class=rb>41929</td>
```

由于只需要分析两列数据，所以我做一个切割替换

```bash
sed -E -i 's/^<tr><td class=rb>.*<\/td><td class=rb>(\d*)<\/td><td class=rb>(\d*)<\/td><td class=rb>.*$/\1\s\2/g' web.data
```

排序即可

